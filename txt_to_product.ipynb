{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YFeFVlZeYFO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kv3ODyLetOk"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/AI_Lec_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyRUytwue2nq"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install docx\n",
        "!pip install exceptions\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsYERi7zeQev"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "이 코드는 텍스트 파일을 처리하기 위한 파이썬 스크립트입니다.\n",
        "\n",
        "사용법:\n",
        "1. 환경 변수 설정: .env 파일에 OpenAI API 키를 설정하세요. \"OPEN_API_KEY=your-api-key\" 형식을 따릅니다.\n",
        "2. raw_root_dir 설정: 'raw_root_dir' 변수에 원본 텍스트 파일이 위치한 루트 디렉토리의 경로를 문자열로 설정합니다.\n",
        "   예) raw_root_dir = 'C:\\\\Users\\\\username\\\\Documents\\\\raw_texts'\n",
        "\n",
        "요구되는 파일 포맷:\n",
        "- 파일은 .txt 확장자를 가져야 합니다.\n",
        "- 파일명은 'raw_subject_week_day_numb.txt' 형태를 따라야 합니다. 각 항목은 다음과 같습니다.\n",
        "  - raw: 파일이 원시 텍스트임을 나타냅니다. 이 접두사가 없는 파일은 무시됩니다.\n",
        "  - subject: 과목명입니다. 예) \"math\", \"physics\", \"history\"\n",
        "  - week: 해당 주차를 나타냅니다. 예) \"week1\", \"week2\"\n",
        "  - day: 해당 일자를 나타냅니다. 예) \"day1\", \"day2\"\n",
        "  - numb: 번호를 나타냅니다. 예) \"1\", \"2\"\n",
        "\n",
        "코드 작동 방식:\n",
        "1. get_txt_files 함수는 raw_root_dir에서 모든 .txt 파일의 전체 경로를 찾습니다.\n",
        "2. raw_text_to_product 함수는 각 텍스트 파일을 처리하고, 이를 여러 형태로 변환합니다.\n",
        "\n",
        "파일 저장 위치:\n",
        "- 중간 결과물은 'intermediate' 디렉토리에 'subject_week_day_numb_paragraphed.txt' 형태로 저장됩니다.\n",
        "- 교정된 텍스트는 'intermediate' 디렉토리에 'subject_week_day_numb_corrected.txt' 형태로 저장됩니다.\n",
        "- 영어로 번역된 텍스트는 'englished' 디렉토리에 'subject_week_day_numb_englished.txt' 형태로 저장됩니다.\n",
        "- 요약된 텍스트는 'summary' 디렉토리에 'subject_week_day_numb_summarised.txt' 형태로 저장됩니다.\n",
        "\n",
        "디렉토리가 없을 경우 자동으로 생성됩니다.\n",
        "\"\"\"\n",
        "# 코드 시작\n",
        "\n",
        "from correct_gpt_v3 import *\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Initialize environment variables\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPEN_API_KEY\")\n",
        "\n",
        "raw_root_dir = '/content/drive/MyDrive/raw' # Update this with your actual root directory path\n",
        "used_raw_root_dir = '/content/drive/MyDrive/used_raw'\n",
        "\n",
        "def get_txt_files(root_dir):\n",
        "    print(\"Scanning for TXT files...\")\n",
        "    txt_files = []\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                txt_files.append(full_path)\n",
        "    print(f\"Found {len(txt_files)} TXT files.\")\n",
        "    return txt_files\n",
        "\n",
        "\n",
        "def raw_text_to_product(raw_text_path, subject, week, day, numb):\n",
        "    print(f\"Processing: {raw_text_path}\")\n",
        "    base_name = f'{subject}_{week}_{day}_{numb}'\n",
        "\n",
        "    for dir_name in ['intermediate', 'englished', 'summary']:\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.makedirs(dir_name)\n",
        "\n",
        "    print(\"Converting to paragraphed text...\")\n",
        "    to_paragraphed(\n",
        "        original_file_path=raw_text_path,\n",
        "        output_file_path=os.path.join('intermediate', f'{base_name}_paragraphed.txt')\n",
        "    )\n",
        "\n",
        "    print(\"Correcting the text...\")\n",
        "    to_corrected(\n",
        "        original_file_path=os.path.join('intermediate', f'{base_name}_paragraphed.txt'),\n",
        "        output_file_path=os.path.join('intermediate', f'{base_name}_corrected.txt')\n",
        "    )\n",
        "\n",
        "    print(\"Translating to English...\")\n",
        "    to_englished(\n",
        "        original_file_path=os.path.join('intermediate', f'{base_name}_corrected.txt'),\n",
        "        output_file_path=os.path.join('englished', f'{base_name}_englished.txt')\n",
        "    )\n",
        "\n",
        "    print(\"Creating summary...\")\n",
        "    to_summarised(\n",
        "        original_file_path=os.path.join('englished', f'{base_name}_englished.txt'),\n",
        "        output_file_path=os.path.join('summary', f'{base_name}_summarised.txt')\n",
        "    )\n",
        "    print(f\"Processing for {raw_text_path} to word file\")\n",
        "    convert_markdown_to_word(os.path.join('summary', f'{base_name}_summarised.txt'), os.path.join('summary', f'{base_name}_word'))\n",
        "\n",
        "    print(f\"Processing for {raw_text_path} completed.\")\n",
        "    shutil.move(raw_text_path, os.path.join(used_raw_root_dir, f'raw_{base_name}.txt'))\n",
        "    print(f\"{raw_text_path} moved to {os.path.join(used_raw_root_dir, f'raw_{base_name}.txt')}\")\n",
        "\n",
        "\n",
        "def extract_info_from_file_name(file_name):\n",
        "    file_name = file_name[:-4]\n",
        "    components = file_name.split('_')\n",
        "    if components[0] == 'raw':\n",
        "        _, subject, week, day, numb = components\n",
        "        return subject, week, day, numb\n",
        "    else:\n",
        "        return None, None, None, None\n",
        "\n",
        "\n",
        "def full_function():\n",
        "    raw_text_file_list = get_txt_files(raw_root_dir)\n",
        "    for file_path in raw_text_file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        print(file_name)\n",
        "        subject, week, day, numb = extract_info_from_file_name(file_name)\n",
        "        print(subject, week, day, numb)\n",
        "        if subject is not None:\n",
        "            raw_text_to_product(raw_text_path=file_path, subject=subject, week=week, day=day, numb=numb)\n",
        "\n",
        "error_count = 0\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        print('starting full function')\n",
        "        full_function()\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        error_count += 1\n",
        "        if error_count >= 5:\n",
        "            print(\"Too many errors. Exiting.\")\n",
        "            break\n",
        "        print('retrying')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}